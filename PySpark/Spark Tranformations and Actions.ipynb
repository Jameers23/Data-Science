{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad2bec1-0cf9-4e29-b147-5ff2e2b672b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-742308498988892>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n",
       "\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    200\u001B[0m     )\n",
       "\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n",
       "\u001B[1;32m    205\u001B[0m         master,\n",
       "\u001B[1;32m    206\u001B[0m         appName,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n",
       "\u001B[1;32m    217\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n",
       "\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n",
       "\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    493\u001B[0m             currentAppName,\n",
       "\u001B[1;32m    494\u001B[0m             currentMaster,\n",
       "\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n",
       "\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n",
       "\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n",
       "\u001B[1;32m    498\u001B[0m         )\n",
       "\u001B[1;32m    499\u001B[0m     )\n",
       "\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-742308498988892>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa078433-af86-4454-8172-1d7105ca5676",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squares of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] is: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "numbers_rdd = sc.parallelize([i for i in range(11)]) #sc.parallelize() is used to parallelize a Python collection into an RDD\n",
    "square_rdd = numbers_rdd.map(lambda x: x**2) #map transformation is applied to each element of the RDD \n",
    "                                            #The map(func) transformation in PySpark is a fundamental operation that allows you to apply a function to each element in an RDD. The function specified by func is applied to each element, and the result is a new RDD where each element is the result of applying the function to the original elements.\n",
    "square_numbers = square_rdd.collect()   #collect() action is used to retrieve all elements of the RDD\n",
    "print(\"Squares of\", numbers_rdd.collect(), \"is:\", square_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a27362-3d01-4cdc-9d26-c54e678b4c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [('Hello,', 6),\n ('World!!', 7),\n ('Welcome', 7),\n ('to', 2),\n ('the', 3),\n ('PySpark', 7),\n ('World', 5),\n ('You', 3),\n ('are', 3),\n ('on', 2),\n ('the', 3),\n ('way', 3),\n ('to', 2),\n ('become', 6),\n ('a', 1),\n ('Data', 4),\n ('Engineer', 8)]"
     ]
    }
   ],
   "source": [
    "#The flatMap(func) transformation in PySpark is indeed similar to map(func), but with a key difference: it can produce zero, one, or more output items for each input item. This makes flatMap particularly useful when you have operations that might result in an RDD with a different number of elements than the original.\n",
    "\n",
    "sentences_rdd = sc.parallelize(\n",
    "    [\n",
    "        \"Hello, World!!\",\n",
    "        \"Welcome to the PySpark World\",\n",
    "        \"You are on the way to become a Data Engineer\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "def numberOfWords(sentence):\n",
    "    words = sentence.split()\n",
    "    res = []\n",
    "    for word in words:\n",
    "        res.append((word, len(word)))\n",
    "    return res\n",
    "\n",
    "len_rdd = sentences_rdd.flatMap(numberOfWords)\n",
    "len_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1dbd28-0fb6-4dbd-8390-69af065e514c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello,', 6), ('World!!', 7), ('Welcome', 7), ('to', 2), ('the', 3), ('PySpark', 7), ('World', 5), ('You', 3), ('are', 3), ('on', 2), ('the', 3), ('way', 3), ('to', 2), ('become', 6), ('a', 1), ('Data', 4), ('Engineer', 8)]\n"
     ]
    }
   ],
   "source": [
    "#The flatMap(func) transformation in PySpark is indeed similar to map(func), but with a key difference: it can produce zero, one, or more output items for each input item. This makes flatMap particularly useful when you have operations that might result in an RDD with a different number of elements than the original.\n",
    "\n",
    "sentences_rdd = sc.parallelize(\n",
    "    [\n",
    "        \"Hello, World!!\",\n",
    "        \"Welcome to the PySpark World\",\n",
    "        \"You are on the way to become a Data Engineer\"\n",
    "    ]\n",
    ")\n",
    "len_rdd = sentences_rdd.flatMap(lambda x: [(word, len(word)) for word in x.split()])\n",
    "print(len_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9879e9-44ed-4673-be29-1a778b76466f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: ['#PySpark', '#BigData', '#Spark', '#Scala', '#Spark']"
     ]
    }
   ],
   "source": [
    "tweet_rdd = sc.parallelize(\n",
    "    [\n",
    "        \"Learning #PySpark is fun! #BigData\",\n",
    "        \"#Spark + #Scala = Powerful combination\",\n",
    "        \"Structured Streaming with #Spark\"\n",
    "    ]\n",
    ")\n",
    "#getting the hastags from the above tweet\n",
    "hastag_rdd = tweet_rdd.flatMap(lambda x: [i for i in x.split() if i.startswith('#')])\n",
    "hastag_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6582056a-62d9-4e24-8139-97a8cdbec8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]\n"
     ]
    }
   ],
   "source": [
    "#filter(func)\n",
    "numbers_rdd = sc.parallelize([i for i in range(1,101)])\n",
    "even_rdd = numbers_rdd.filter(lambda x: x%2 == 0)\n",
    "print(even_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c2f4d1-1978-4c6f-9730-0ece4901b02f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [4, 8]\n2: [5]\n3: [1]\n"
     ]
    }
   ],
   "source": [
    "#In PySpark, the groupByKey() transformation is used to group all the values associated with each key in the RDD into a single sequence. This can be particularly useful when you want to perform aggregations or other operations on grouped data.\n",
    "\n",
    "groups_rdd = sc.parallelize([(1, 4),(1, 8), (2, 5), (3, 1)])\n",
    "groups_key_rdd = groups_rdd.groupByKey()\n",
    "for key, value in groups_key_rdd.collect():\n",
    "    print(f\"{key}: {list(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed720b98-f6ee-438c-a47c-3f4098ddf6e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [4, 8]\n2 [5]\n3 [1]\n"
     ]
    }
   ],
   "source": [
    "#In PySpark, the groupByKey() transformation is used to group all the values associated with each key in the RDD into a single sequence. This can be particularly useful when you want to perform aggregations or other operations on grouped data.\n",
    "\n",
    "groups_rdd = sc.parallelize([(1, 4),(1, 8), (2, 5), (3, 1)])\n",
    "groups_key_rdd = groups_rdd.groupByKey()\n",
    "for key, value in groups_key_rdd.collect():\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2814bb8-08f8-4e3b-80de-cb53e073dcb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [2, 8]\n3: [4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Assume we have the following RDD with key-value pairs\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (1, 8)])\n",
    "\n",
    "# Applying groupByKey() to group values by their keys\n",
    "grouped_rdd = rdd.groupByKey()\n",
    "\n",
    "# To see the results\n",
    "for key, values in grouped_rdd.collect():\n",
    "    print(f\"{key}: {list(values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00515007-6590-4a71-86a4-93c18a042d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [2, 8], 3: [4, 6]}\n"
     ]
    }
   ],
   "source": [
    "# Assume we have the following RDD with key-value pairs\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (1, 8)])\n",
    "\n",
    "# Applying groupByKey() to group values by their keys\n",
    "grouped_rdd = rdd.groupByKey().mapValues(list).collectAsMap()\n",
    "print(grouped_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8e2667-2776-4855-b1ee-7a71f15e2057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : ['apple', 'axe', 'apron', 'accenture']\nb : ['ball', 'bat', 'box']\nc : ['cat', 'cow']\nd : ['dog', 'donkey']\n"
     ]
    }
   ],
   "source": [
    "word_rdd = sc.parallelize([\"apple\", \"axe\", \"apron\", \"accenture\", \"ball\", \"bat\", \"box\", \"cat\", \"cow\", \"dog\", \"donkey\"])\n",
    "pair_rdd = word_rdd.map(lambda word: (word[0], word))\n",
    "group_by_start = pair_rdd.groupByKey()\n",
    "for key, val in group_by_start.collect():\n",
    "    print(key, \":\", list(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b777c1-cd75-484f-92fa-7fd54b06c340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2, 8]\n1 [1, 1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "# Perform groupBy operation\n",
    "result = rdd.groupBy(lambda x: x % 2)\n",
    "# Print the result\n",
    "for key, val in result.collect():\n",
    "    print(key, list(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aedcc030-3c34-4802-9a92-ab40c0cf715d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [2, 8], 1: [1, 1, 3, 5]}\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "# Perform groupBy operation\n",
    "result = rdd.groupBy(lambda x: x % 2).mapValues(list).collectAsMap()\n",
    "        #mapValues(list) call converts the grouped values into lists.\n",
    "        #collectAsMap() is used to collect the result as a dictionary.\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79759660-720f-4e2e-8efc-bee08cd2d68d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': ['grape'], 'a': ['apple', 'apple'], 'b': ['banana', 'banana'], 'o': ['orange']}\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([\"apple\", \"banana\", \"orange\", \"apple\", \"grape\", \"banana\"])\n",
    "# Perform groupBy operation\n",
    "result = rdd.groupBy(lambda x: x[0]).mapValues(list).collectAsMap()\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496e34af-2b14-4999-ae32-a80b06b7160b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: [('orange', 4), ('apple', 4), ('banana', 3)]"
     ]
    }
   ],
   "source": [
    "#reduceByKey() transformation is used to merge the values of each key using an associative reduce function on an RDD.\n",
    "word_freq = sc.parallelize([(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"banana\", 1), (\"orange\", 4)])\n",
    "word_freq_total = word_freq.reduceByKey(lambda x, y: x + y)\n",
    "word_freq_total.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93baa08e-5a68-49dd-82f2-0f07b341c7e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: p2, Maximum Sale: 400, Number of Sales: 2\nProduct: p3, Maximum Sale: 100, Number of Sales: 1\nProduct: p1, Maximum Sale: 300, Number of Sales: 2\n"
     ]
    }
   ],
   "source": [
    "#aggregateByKey() transformation is used to aggregate the values of each key using custom combine functions and an initial “zero value.” \n",
    "\n",
    "\n",
    "# Create an RDD of tuples representing sales data\n",
    "sales_data = [(\"p1\", 100), (\"p2\", 200), (\"p1\", 300), (\"p2\", 400), (\"p3\", 100)]\n",
    "\n",
    "# Parallelize the data to create an RDD\n",
    "sales_rdd = sc.parallelize(sales_data)\n",
    "\n",
    "# Define zeroValue\n",
    "zero_value = (0, 0) # (max sale, number of sales)\n",
    "\n",
    "# Define seqFunc\n",
    "def seqFunc(accumulator, element):\n",
    "    return (max(accumulator[0], element), accumulator[1] + 1)\n",
    "\n",
    "# Define combFunc\n",
    "def combFunc(accumulator1, accumulator2):\n",
    "    return (max(accumulator1[0], accumulator2[0]), accumulator1[1] + accumulator2[1])\n",
    "\n",
    "# Use aggregateByKey to calculate the maximum sale and total number of sales per product\n",
    "aggregated_sales_rdd = sales_rdd.aggregateByKey(zero_value, seqFunc, combFunc)\n",
    "\n",
    "# Collect and print the results\n",
    "results = aggregated_sales_rdd.collect()\n",
    "for (product, (max_sale, num_sales)) in results:\n",
    "    print(f\"Product: {product}, Maximum Sale: {max_sale}, Number of Sales: {num_sales}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15808d34-843b-4263-abac-62371d6ad0a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student ID: S2, Average Score: 87.66666666666667, Total Subjects: 3\nStudent ID: S1, Average Score: 77.5, Total Subjects: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample data: (student_id, (subject, score))\n",
    "scores_data = [\n",
    "    (\"S1\", (\"Math\", 80)),\n",
    "    (\"S1\", (\"Physics\", 75)),\n",
    "    (\"S2\", (\"Math\", 90)),\n",
    "    (\"S2\", (\"Physics\", 85)),\n",
    "    (\"S2\", (\"Chemistry\", 88))\n",
    "]\n",
    "\n",
    "# Parallelize the data to create an RDD\n",
    "scores_rdd = sc.parallelize(scores_data)\n",
    "\n",
    "# Define zeroValue\n",
    "zero_value = (0, 0)  # (total score, total subjects)\n",
    "\n",
    "# Define seqFunc\n",
    "def seqFunc(accumulator, element):\n",
    "    subject, score = element\n",
    "    return (accumulator[0] + score, accumulator[1] + 1)\n",
    "\n",
    "# Define combFunc\n",
    "def combFunc(accumulator1, accumulator2):\n",
    "    return (accumulator1[0] + accumulator2[0], accumulator1[1] + accumulator2[1])\n",
    "\n",
    "# Use aggregateByKey to calculate the average score and total number of subjects per student\n",
    "average_scores_rdd = scores_rdd.aggregateByKey(zero_value, seqFunc, combFunc) \\\n",
    "                                .mapValues(lambda accum: (accum[0] / accum[1], accum[1]))\n",
    "\n",
    "# Collect and print the results\n",
    "results = average_scores_rdd.collect()\n",
    "for student_id, (average_score, total_subjects) in results:\n",
    "    print(f\"Student ID: {student_id}, Average Score: {average_score}, Total Subjects: {total_subjects}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ac5e33-63c7-4575-8f7a-440cfd7531a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [2, 3, 4, 5, 6]"
     ]
    }
   ],
   "source": [
    "#The distinct() method removes duplicate elements from an RDD.\n",
    "rdd = sc.parallelize([2,3,4,5,2,4,6])\n",
    "rdd_new = rdd.distinct()\n",
    "rdd_new.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b91bd5-5ad8-4023-a573-dbdcfa7ec34a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [('a', 3), ('a', 1), ('b', 5), ('c', 4)]"
     ]
    }
   ],
   "source": [
    "#sortByKey() is a transformation operation in PySpark that sorts the elements of an RDD.\n",
    "rdd = sc.parallelize([('b', 5), ('a', 3), ('c', 4), ('a', 1)])\n",
    "new_rdd = rdd.sortByKey()\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1159f87f-58b5-49b8-b878-4960c1464d83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [('c', 4), ('b', 5), ('a', 3), ('a', 1)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('b', 5), ('a', 3), ('c', 4), ('a', 1)])\n",
    "new_rdd = rdd.sortByKey(ascending=False)\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1260ee9b-e74b-49df-9c36-008380970efa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#The count() method returns the total number of elements in an RDD.\n",
    "rdd = sc.parallelize([('b', 5), ('a', 3), ('c', 4), ('a', 1)])\n",
    "length = rdd.count()\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8fe4a6-8035-4e35-97fc-a68b33fdadd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'b': 1, 'a': 2, 'c': 1})\n"
     ]
    }
   ],
   "source": [
    "#The countByKey() method is used on RDDs containing key-value pairs (usually tuples).\n",
    "rdd = sc.parallelize([('b', 5), ('a', 3), ('c', 4), ('a', 1)])\n",
    "length = rdd.countByKey()\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a2d2b6-2c50-4106-9aff-9d5b7f955960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: defaultdict(int, {('b', 5): 1, ('a', 3): 1, ('c', 4): 1, ('a', 1): 1})"
     ]
    }
   ],
   "source": [
    "#The countByValue() method counts the occurrences of each unique value in the RDD.\n",
    "rdd = sc.parallelize([('b', 5), ('a', 3), ('c', 4), ('a', 1)])\n",
    "length = rdd.countByValue()\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a2a202-cfc2-44fe-940a-fe72e693fdab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [1, 2, 3, 4, 5, 6, 7, 8, 9]"
     ]
    }
   ],
   "source": [
    "#union(otherRDD): Returns a new RDD containing all elements from the source RDD and another RDD.\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([6,7,8,9])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e566ab-2c95-432a-bfb0-42d3b57c1e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [3, 5]"
     ]
    }
   ],
   "source": [
    "#intersection(otherRDD): Returns a new RDD containing only elements found in both source and another RDD.\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([6,3,5,9])\n",
    "rdd3 = rdd1.intersection(rdd2)\n",
    "rdd3.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Tranformations and Actions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
