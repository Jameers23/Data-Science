Artificial Intelligence - making computers to think themselves and take a decision
Artificial General Intelligence (AGI) - making machines as intellent as we are

Machine Learing - it is a sub-field of AI, making computers to learn without being explicitly programmed
	Field of study that gives computers the ability to learn without being explicitly programmed.
	
Applications - 
	1. hospitals to cure diseases
	2. computer vision for object detection
	3. in factories, to identify the defect products
	4. voice to text conversion
	5. search engines to rank the webpages
	6. recommendation systems in netflix, amazon etc.
	7. email spam and ham classification
	
Major Machine Learning Techniques:
	1. Regression or Estimation: Predicting continuous values
	2. Classification: Predicting the item class/category of a case
	3. Clustering: Finding the structure of data; summaization
	4. Associations: Associating frequent co-occurring items/events
	5. Anomaly Detection: Discovering abnormal and unusual cases
	6. Sequence Mining: Predicting next events
	7. Dimension Reduction: Reducing the size of data
	8. Recommendation Systems: Recommending items
	
Difference AI, ML and DL:
	AI - tries to make computers intelligent to mimic the cognitive functions of humans.
		Eg: Computer Vision, Language Processing, Creativity etc
	ML - branch of AI that covers the statistical part of AI, it teaches the computer to solve problems by looking at 100's or 1000's of examples, using the experience to solve the next examples
		Eg: Classification, Regression, Clustering etc
	DL: field of ML where computers can learn and make intelligent decisions on their own. 
		Eg: Object Detection, Neural Networks etc
	
Python Libraries to be used:
	1. NumPy: used to work with n-dimensional arrays, enables to do compution effeciently and effectively.
	2. SciPy - collection of numerical algorithms
	3. Matplotlib - used for visualizing data
	4. Pandas - used for high dimensional datastructures such as DataFrames
	5. Scikit -learn - provides ML algorithms

Machine Learning Algorithms are of mainly two types:
	1. Supervised Learning
	2. Unsupervised Learning
	
Supervised Learning: Labelled Data
	input --> outputs
	learns from data labeled with right answers
	trains the algorithm by giving the right answers for inputs
	Eg:
		- spam filtering
		- speech recognition
		- language translation
		- online advertising
		- online advertising
		- self-driving car
		- visual inspection 
	
	Regression:
		Predict a number from infinitely many possible outputs
		process of predicting comtinuous variables
		
	Classification: 
		predicts categories from limited and finite set of categories
		
Unsupervised Learning: Unlabelled Data
		data only comes with inputs, but not with outputs
		algorithm has to find structure in the data.
	1. Clustering: collection of group of the data similar in structure
		group similar data points together
		Application: Google News
	2. Anomaly Detection: find unsual data points
		Eg: Fraud detection
	3. Dimensionality Reduction: compress data using fewer numbers
	4. Market Basket Analysis
	5. Density Estimation
	
Terminology:
	training data - data used to train the model, d
		notation-
			'x' (input variable or feature variable) 
			'y' (output or target variable)
			'm' (number of training examples)
	
	
Linear Regression:
	supervised regression algorithm
	supervised since the labelled data will be fed to the model
	regression is due to it predicts the numbers
		
		training data --> learning algorithm --> f (function/model) --> y^ (y-hat) (estimated value for the input x)
		
			f(x) = wx+b    i.e., w,b are parameters (or coefficients or weights)
			b is called as y-intercept
			w is slope
			
			linear is nothing but a straight line
			
			linear regression with one variable is univariate linear regression 
									two variables is bivariate
									multiple variables is mutli-variate
									
			cost function: squared error cost function
				J = (1/2m)sigma(y^ - y)2
									
									
		model: fw,b(x) = wx+b
		parameters: w,b
		cost function: 
			J(w,b) = (1/2m)sigma(y^ - y)2
			J(w,b) = (1/2m)sigma(fw,b(x) - y)2
		goal: minimize J(w,b)
		
		simplified:
			model: f(x) = wx (b=0)
		parameters: w
		cost function: 
			J(w) = (1/2m)sigma(y^ - y)2
			J(w) = (1/2m)sigma(fw(x) - y)2
		goal: minimize J(w)
		
		the algorithm that is used to find the minimum cost function is gardient descent
		
		have some function J(w,b)
		want min J(w,b)
		
		Outline:
			start with some w, b (set w=0, b=0 as initial guess)
			keep changing w,b to reduce J until we settle at or near a minimum
			
		w = w - (alpha) d/dw J(w,b)
			alpha is learning rate (lies between 0 and 1)
			
			d/dw J(w,b) is derivate of cost function
		
		b = b - (alpha) d/db J(w,b)
		
		if alpha is too small then gradient descent may be slow
		
		if alpha is too large then the gradient may 
			- overshoot, never reach minimum
			- fail to converge, diverge
	
Feature Engineering: Using inituition to design new features, by transforming or combining original features.
	